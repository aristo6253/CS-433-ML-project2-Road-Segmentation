{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "sys.path.append('../')\n",
    "from tools import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "root_dir = \"../data/training_augmented/\"\n",
    "img_dir = root_dir + \"images/\"\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "\n",
    "files = os.listdir(img_dir)\n",
    "\n",
    "n = min(50, len(files)) # Load maximum 20 images\n",
    "print(\"Loading \" + str(n) + \" images and their labels...\")\n",
    "imgs = [load_image(img_dir + files[i]) for i in range(n)]\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "\n",
    "# Split into training and test set\n",
    "X = np.array(imgs)\n",
    "y = np.array(gt_imgs)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "n_train = X_train.shape[0]\n",
    "n_val = X_val.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from input images\n",
    "patch_size = 16\n",
    "\n",
    "X_train_patches = [img_crop(X_train[i], patch_size, patch_size) for i in range(n_train)]\n",
    "y_train_patches = [img_crop(y_train[i], patch_size, patch_size) for i in range(n_train)]\n",
    "\n",
    "X_val_patches = [img_crop(X_val[i], patch_size, patch_size) for i in range(n_val)]\n",
    "y_val_patches = [img_crop(y_val[i], patch_size, patch_size) for i in range(n_val)]\n",
    "\n",
    "# Linearize list of patches (make it a 2D array)\n",
    "X_train_patches = np.asarray([X_train_patches[i][j] for i in range(len(X_train_patches)) for j in range(len(X_train_patches[i]))])\n",
    "y_train_patches = np.asarray([y_train_patches[i][j] for i in range(len(y_train_patches)) for j in range(len(y_train_patches[i]))])\n",
    "\n",
    "X_val_patches = np.asarray([X_val_patches[i][j] for i in range(len(X_val_patches)) for j in range(len(X_val_patches[i]))])\n",
    "y_val_patches = np.asarray([y_val_patches[i][j] for i in range(len(y_val_patches)) for j in range(len(y_val_patches[i]))])\n",
    "\n",
    "# Compute the features\n",
    "X_train_features = np.asarray([extract_features(X_train_patches[i]) for i in range(len(X_train_patches))])\n",
    "y_train_features = np.asarray([value_to_class(np.mean(y_train_patches[i])) for i in range(len(y_train_patches))])\n",
    "\n",
    "X_val_features = np.asarray([extract_features(X_val_patches[i]) for i in range(len(X_val_patches))])\n",
    "y_val_features = np.asarray([value_to_class(np.mean(y_val_patches[i])) for i in range(len(y_val_patches))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature statistics\n",
    "\n",
    "print(\"Computed \" + str(X_train_features.shape[0]) + \" features\")\n",
    "print(\"Feature dimension = \" + str(X_train_features.shape[1]))\n",
    "print(\"Number of classes = \" + str(np.max(y_train_features)))  # TODO: fix, length(unique(Y))\n",
    "\n",
    "Y0 = [i for i, j in enumerate(y_train_features) if j == 0]\n",
    "Y1 = [i for i, j in enumerate(y_train_features) if j == 1]\n",
    "print(\"Class 0: \" + str(len(Y0)) + \" samples, \" + str(100 * len(Y0) / len(y_train_features)) + \"%\")\n",
    "print(\"Class 1: \" + str(len(Y1)) + \" samples, \" + str(100 * len(Y1) / len(y_train_features)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM classifier\n",
    "print(\"Training SVM classifier...\")\n",
    "svm_model = SVC(kernel='rbf', class_weight='balanced', C=1, gamma='auto')\n",
    "svm_model.fit(X_train_features, y_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training set\n",
    "print(\"Predicting on the training set...\")\n",
    "y_train_pred = svm_model.predict(X_train_features)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "y_train_pred_nz = np.nonzero(y_train_pred)[0]\n",
    "y_train_features_nz = np.nonzero(y_train_features)[0]\n",
    "\n",
    "TPR = len(list(set(y_train_features_nz) & set(y_train_pred_nz))) / float(len(y_train_pred))\n",
    "print(\"True Positive Rate = \" + str(TPR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "print(\"Predicting on the validation set...\")\n",
    "y_val_pred = svm_model.predict(X_val_features)\n",
    "\n",
    "# Compute accuracy on the validation set\n",
    "y_val_pred_nz = np.nonzero(y_val_pred)[0]\n",
    "y_val_features_nz = np.nonzero(y_val_features)[0]\n",
    "\n",
    "TPR = len(list(set(y_val_features_nz) & set(y_val_pred_nz))) / float(len(y_val_pred))\n",
    "print(\"True Positive Rate = \" + str(TPR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "print(\"Creating submission file...\")\n",
    "test_dir = \"../data/test_set_images/\"\n",
    "\n",
    "# Create directory called prediction_logreg under ../data/test_set_images/\n",
    "if not os.path.exists(\"../predictions/prediction_svm/\"):\n",
    "    os.makedirs(\"../predictions/prediction_svm/\")\n",
    "\n",
    "\n",
    "for file in os.listdir(test_dir):\n",
    "    print(\"Processing \" + file + \"...\")\n",
    "    img_path = os.path.join(test_dir, file, f'{file}.png')\n",
    "\n",
    "    img = load_image(img_path)\n",
    "    img_patches = img_crop(img, patch_size, patch_size)\n",
    "    img_patches = np.asarray(img_patches)\n",
    "    img_patches_features = np.asarray([extract_features(img_patches[i]) for i in range(len(img_patches))])\n",
    "    \n",
    "    img_patches_pred = svm_model.predict(img_patches_features)\n",
    "    img_pred = label_to_img(img.shape[0], img.shape[1], patch_size, patch_size, img_patches_pred)\n",
    "    img_pred = img_float_to_uint8(img_pred)\n",
    "\n",
    "    img_pred_path = os.path.join(\"../predictions/prediction_svm/\", f'{file}.png')\n",
    "    img_pred_img = Image.fromarray(img_pred)\n",
    "    img_pred_img.save(img_pred_path)\n",
    "\n",
    "    cimg_pred = concatenate_images(img, img_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission_filename = '../predictions/svm_submission.csv'\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = '../predictions/prediction_svm/test_' + str(i) + '.png'\n",
    "    print(image_filename)\n",
    "    image_filenames.append(image_filename)\n",
    "masks_to_submission(submission_filename, *image_filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
